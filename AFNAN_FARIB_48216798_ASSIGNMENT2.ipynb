{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13978f4a-b4fa-4959-a6d6-69ae52f58359",
   "metadata": {},
   "source": [
    "## NAME: AFNAN FARIB\n",
    "## STUDENT ID: 48216798\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52314ed0",
   "metadata": {},
   "source": [
    "# Assignment 2: Python for Text Processing\n",
    "\n",
    "**Submission deadline:** Friday, 31 Oct 2025, 11:55 PM  \n",
    "**Assessment marks:** 35 marks (35% of the total unit assessment)\n",
    "\n",
    "---\n",
    "\n",
    "### Late Submission Penalty\n",
    "\n",
    "Unless a Special Consideration request has been submitted and approved, a 5% penalty (of the total possible mark of the task) will be applied for each day a written report or presentation assessment is not submitted, up until the 7th day (including weekends). After the 7th day, a grade of ‘0’ will be awarded even if the assessment is submitted.\n",
    "\n",
    "> **Example:** If the assignment is worth 8 marks (of the entire unit) and your submission is late by 19 hours (or 23 hours 59 minutes 59 seconds), 0.4 marks (5% of 8 marks) will be deducted. If your submission is late by 24 hours (or 47 hours 59 minutes 59 seconds), 0.8 marks (10% of 8 marks) will be deducted, and so on.\n",
    "\n",
    "The submission time for all uploaded assessments is **11:55 PM**. A **1-hour grace period** is provided for technical concerns.  Apply for [Special Consideration](https://students.mq.edu.au/study/assessment-exams/special-consideration), if you think you should be granted an extended deadline or waive the late submission penalty. You should apply immediately when the situation occurs.\n",
    "\n",
    "---\n",
    "\n",
    "### Academic Integrity\n",
    "\n",
    "All submitted work must be your own. For rules around AI tools, refer to **\"Using Generative AI Tools\" on iLearn**.\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Complete the five tasks below.\n",
    "\n",
    "* Write your code and comments inside this notebook.\n",
    "\n",
    "* Your notebook must include the running outputs of your final code.\n",
    "\n",
    "* **Submit this `.ipynb` file, containing your code and outputs, to iLearn.**\n",
    "\n",
    "---\n",
    "\n",
    "### Assessment\n",
    "\n",
    "-  Marks are based on the correctness of your code, outputs, and coding style.\n",
    "-  A total of **2.5 marks** (0.5 per task) are awarded globally across the assignment for both of the below: (1) runnable codes; (2) good coding style: clean, modular code, meaningful variable names, and good comments.\n",
    "-  If outputs are missing or incorrect, up to **25% of the marks for that task** can be deducted.\n",
    "-  See each task below for the detailed mark breakdown.\n",
    "\n",
    "---\n",
    "\n",
    "### AI Tools Usage Policy\n",
    "\n",
    "\n",
    "In this assignment, we view AI code generators such as copilot, CodeGPT, etc as tools that can help you write code quickly. You are allowed to use these tools, but with some conditions. To understand what you can and what you cannot do, please visit these information pages provided by Macquarie University.\n",
    "\n",
    "- See: [Artificial Intelligence Tools and Academic Integrity in FSE](https://bit.ly/3uxgQP4)\n",
    "\n",
    "If you choose to use these tools, make the following explicit in your submitted file as comments starting with \"Use of AI generators in this assignment\" :\n",
    "\n",
    "- What part of your code is based on the output of such tools,\n",
    "- What tools you used,\n",
    "- What prompts you used to generate the code or text, and\n",
    "- What modifications you made on the generated code or text?\n",
    "\n",
    "This will help us assess your work fairly. \n",
    "\n",
    "**If we observe that you have used an AI generator and you do not give the above information, you may face disciplinary action.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bbb19-fa1b-4d15-a2a4-541b7c6c7722",
   "metadata": {},
   "source": [
    "## USE OF AI GENERATOR IN THIS ASSIGNMENT\n",
    "AI tools helped with question 4 which is building the Siamese Neural Network and Question 5 which is fine-tuning the transformer model as I had to reduce the batch number size to run the code on my device. \n",
    "\n",
    "These tools include OpenAI's ChatGPT and Hugging Face Transformers, which were used to create and improve the code.\n",
    "\n",
    "Prompt used involved requests for model architectures, training procedures, and resource optimization strategies.\n",
    "\n",
    "Modifications made include adapting the generated code to fit the specific dataset, resource constraints, and fixing errors encountered during implementation.\n",
    "\n",
    "This disclosure is made to comply with Macquarie University's AI usage policy and to ensure fair assessment of my work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6c533",
   "metadata": {},
   "source": [
    "\n",
    "## Objectives of this assignment\n",
    "\n",
    "In this assignment, you will work on the Quora Question Pairs (QQP) datset detailed below. The first two tasks will help you get familiar with the data, and the remaining requires you to implement deep neural networks.\n",
    "\n",
    "\n",
    "**About the Quora Question Pairs (QQP) Dataset**\n",
    "\n",
    "Description: A large dataset of 400k+ question pairs from Quora, labeled whether they are duplicates (semantically the same) or not. It features informal, noisy text with class imbalance, hard positives (low lexical overlap) and hard negatives (high overlap, different meaning). QQP is practically relevant for deduplicating FAQs, search, and support systems. Working on QQP builds transferable skills, such as text preprocessing, model comparison, threshold tuning, error analysis, and deployment-minded reasoning about real applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4113f16",
   "metadata": {},
   "source": [
    "**Get familiar with the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f881ff1-728c-44b0-b20a-0f9282be8e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Collecting dill<0.4.1,>=0.3.0\n",
      "  Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.0.1)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0\n",
      "  Downloading huggingface_hub-1.0.0-py3-none-any.whl (503 kB)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Collecting pyarrow>=21.0.0\n",
      "  Downloading pyarrow-21.0.0-cp39-cp39-win_amd64.whl (26.2 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.6.0-cp39-cp39-win_amd64.whl (31 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.9.0,>=2023.1.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.13.1-cp39-cp39-win_amd64.whl (454 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.22.0-cp39-cp39-win_amd64.whl (87 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.7.0-cp39-cp39-win_amd64.whl (46 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.4.1-cp39-cp39-win_amd64.whl (41 kB)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.8.0-cp39-cp39-win_amd64.whl (44 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1.0.0->datasets) (2024.7.4)\n",
      "Requirement already satisfied: idna in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1.0.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Collecting shellingham\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "Collecting typer-slim\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.2.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: propcache, multidict, frozenlist, yarl, click, async-timeout, aiosignal, aiohappyeyeballs, typer-slim, tqdm, shellingham, hf-xet, dill, aiohttp, xxhash, pyarrow, multiprocess, huggingface-hub, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.1 aiosignal-1.4.0 async-timeout-5.0.1 click-8.1.8 datasets-4.3.0 dill-0.4.0 frozenlist-1.8.0 hf-xet-1.2.0 huggingface-hub-1.0.0 multidict-6.7.0 multiprocess-0.70.16 propcache-0.4.1 pyarrow-21.0.0 shellingham-1.5.4 tqdm-4.67.1 typer-slim-0.20.0 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b74181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip -q install datasets    # Install the datasets package to access the dataset\n",
    "# add the packages you used, and specify the verion you installed\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55f743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\huggingface_hub\\file_download.py:120: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Afnan Farib\\.cache\\huggingface\\hub\\datasets--glue. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|███████████████████████████████████████| 363846/363846 [00:00<00:00, 599095.49 examples/s]\n",
      "Generating validation split: 100%|████████████████████████████████████| 40430/40430 [00:00<00:00, 315031.36 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████| 390965/390965 [00:00<00:00, 515453.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 1) Load QQP\n",
    "ds = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "# Use validation set as our test; optionally create a smaller train subset for speed\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds  = ds[\"validation\"]\n",
    "\n",
    "q1_tr = list(train_ds[\"question1\"])\n",
    "q2_tr = list(train_ds[\"question2\"])\n",
    "y_tr  = np.array(train_ds[\"label\"])\n",
    "\n",
    "q1_te = list(eval_ds[\"question1\"])\n",
    "q2_te = list(eval_ds[\"question2\"])\n",
    "y_te  = np.array(eval_ds[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05743951",
   "metadata": {},
   "source": [
    "### Task 1. What is the top-5 common NOUN in the question1 and question2, respectively? (5 marks)\n",
    "\n",
    "Write codes that returns the top-5 common NOUN in the questions. To find the part of speech, use NLTK's \"Universal\" tag set. You may need to use NLTK's `sent_tokenize` and `word_tokenize` to get words. The function returns a list that is descendingly sorted according to freqency, e.g. [(noun1, 22), (noun2, 10), ...].\n",
    "<!-- To produce the correct results, the function must do this.  -->\n",
    "Hint: The following steps will produce the correct results:\n",
    "\n",
    "- Concatenate all questions together.\n",
    "- Use the NLTK libraries to find the tokens and the stems.\n",
    "- Use NLTK's sentence tokeniser before NLTK's word tokeniser.\n",
    "- Use NLTK's part of speech tagger, using the \"Universal\" tagset.\n",
    "- Use NLTK's `pos_tag_sents` instead of `pos_tag`.\n",
    "\n",
    "Marking Criteria: \n",
    "- 2.5 marks for the correct codes and results of each column, namely question1 and question2 columns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ceed109-61a6-4ef1-b598-aa8f82fd9cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2025.10.23-cp39-cp39-win_amd64.whl (277 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.2 regex-2025.10.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb05fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag_sents\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0bb1b59-136a-4211-9e83-2a96118d99ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Afnan\n",
      "[nltk_data]     Farib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Afnan Farib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f0a45da-5ee0-4831-9732-b4d2ab3a6cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Afnan\n",
      "[nltk_data]     Farib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1699eee-61e8-45c0-b072-50689ef0b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Afnan\n",
      "[nltk_data]     Farib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99ff224a-98ac-4768-9401-f5d7d5649732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Afnan\n",
      "[nltk_data]     Farib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e5485a6-af94-433d-874d-4bbedc2feb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Afnan Farib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Afnan\n",
      "[nltk_data]     Farib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f6ef88e-2cf4-4a8f-8e6b-e117f336c730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 nouns in question1: [('india', 12699), ('people', 11449), ('way', 7695), ('quora', 7678), ('life', 6774)]\n",
      "Top 5 nouns in question2: [('india', 13480), ('people', 12231), ('way', 8615), ('quora', 7967), ('life', 7258)]\n"
     ]
    }
   ],
   "source": [
    "def get_top_nouns(question_list, top_k=5):\n",
    "    # concatenating all questions into a single string\n",
    "    all_text = \" \".join(q for q in question_list if q)\n",
    "    #  Sentence tokenization\n",
    "    sentences = sent_tokenize(all_text)\n",
    "    # Word tokenization per sentence\n",
    "    tokenized_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "    # POS tagging (using 'universal' tagset)\n",
    "    pos_tagged = pos_tag_sents(tokenized_sentences, tagset='universal')\n",
    "    # Collecting all nouns\n",
    "    noun_list = [word.lower() for sent in pos_tagged for (word,tag) in sent if tag == 'NOUN']\n",
    "    # Getting frequencies\n",
    "    counts = Counter(noun_list)\n",
    "    # Return top-k as a list of tuples\n",
    "    return counts.most_common(top_k)\n",
    "\n",
    "# Apply to your data\n",
    "top5_question1 = get_top_nouns(q1_tr)\n",
    "top5_question2 = get_top_nouns(q2_tr)\n",
    "\n",
    "print(\"Top 5 nouns in question1:\", top5_question1)\n",
    "print(\"Top 5 nouns in question2:\", top5_question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ac4b3",
   "metadata": {},
   "source": [
    "### Task 2. What are the top-5 common stem 2-grams and non-stem 2-grams for question1 and question2, respectively? (5 marks)\n",
    "\n",
    "Write codes that returns the top-5 most frequent 2-grams (bigrams) of stemmed and non-stemmed tokens along with their normalized frequency from the question1 and question2 columns of the QQP dataset. The output should be in descending order of frequency, **with frequencies normalized by the total number of bigrams (rounded to 4 decimal places)**, e.g., `[(('what', 'is'), 0.0105), (('what', 'are'), 0.0053), ...]`.\n",
    "\n",
    "<!-- To produce the correct results, the function must do this: -->\n",
    "\n",
    "Hint: The following steps will produce the correct results:\n",
    "\n",
    "- Concatenate all questions together.\n",
    "- Use NLTK's sentence tokeniser before NLTK's word tokeniser.\n",
    "- Use the NLTK libraries to find the tokens and the stems.\n",
    "- Use NLTK's Porter stemmer to get the root words.\n",
    "- Round normalized frequency to 4 precision after the decimal point.\n",
    "- When computing bigrams, do not consider words that are in different sentences. For example, if we have this text: `Sentence 1. And sentence 2.` the bigrams are: `('Sentence','1'), ('1','.'), ('.','And'), ('And','sentence')`, etc. Note that the following would not be a valid bigram, since the punctuation mark and the word \"And\" are in different sentences: `('.','And')`.\n",
    "\n",
    "Marking Criteria: \n",
    "- 2.5 marks for the correct codes and restuls of each column, namely question1 and question2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e151460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19e34cad-1414-4bd4-b0da-6f73201608df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bigrams(question_list, stemmed=False, top_k=5):\n",
    "    # Initializing stemmer if needed\n",
    "    stemmer = PorterStemmer() if stemmed else None\n",
    "    # Concatenating all questions into a single string\n",
    "    all_text = \" \".join(q for q in question_list if q)\n",
    "\n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(all_text)\n",
    "\n",
    "    # Word tokenization per sentence\n",
    "    tokenized_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # Optionally stem tokens\n",
    "    if stemmed:\n",
    "        tokenized_sentences = [[stemmer.stem(token) for token in sent] for sent in tokenized_sentences]\n",
    "\n",
    "    # Make bigrams within each sentence\n",
    "    bigrams = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        bigrams.extend(list(nltk.bigrams(tokens)))\n",
    "\n",
    "    # Normalize frequencies\n",
    "    total_bigrams = len(bigrams)\n",
    "    counts = Counter(bigrams)\n",
    "    results = [(bigrams, round(count/total_bigrams, 4)) for bigrams, count in counts.most_common(top_k) ]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb4fb33f-a20c-4230-8345-b0fc223d446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1 top-5 non-stemmed bigrams: [(('What', 'is'), 0.0125), (('is', 'the'), 0.0108), (('What', 'are'), 0.01), (('How', 'do'), 0.008), (('the', 'best'), 0.0064)]\n",
      "Question2 top-5 non-stemmed bigrams: [(('What', 'is'), 0.012), (('is', 'the'), 0.0104), (('What', 'are'), 0.0096), (('How', 'do'), 0.0081), (('the', 'best'), 0.0066)]\n",
      "Question1 top-5 stemmed bigrams: [(('what', 'is'), 0.0128), (('is', 'the'), 0.0112), (('what', 'are'), 0.0102), (('how', 'do'), 0.0084), (('can', 'i'), 0.0067)]\n",
      "Question2 top-5 stemmed bigrams: [(('what', 'is'), 0.0123), (('is', 'the'), 0.0108), (('what', 'are'), 0.0098), (('how', 'do'), 0.0085), (('can', 'i'), 0.0068)]\n"
     ]
    }
   ],
   "source": [
    "# Most common non-stemmed bigrams\n",
    "top5_bigrams_q1_nonstem = get_top_bigrams(q1_tr, stemmed=False)\n",
    "top5_bigrams_q2_nonstem = get_top_bigrams(q2_tr, stemmed=False)\n",
    "\n",
    "# Most common stemmed bigrams\n",
    "top5_bigrams_q1_stem = get_top_bigrams(q1_tr, stemmed=True)\n",
    "top5_bigrams_q2_stem = get_top_bigrams(q2_tr, stemmed=True)\n",
    "\n",
    "print(\"Question1 top-5 non-stemmed bigrams:\", top5_bigrams_q1_nonstem)\n",
    "print(\"Question2 top-5 non-stemmed bigrams:\", top5_bigrams_q2_nonstem)\n",
    "print(\"Question1 top-5 stemmed bigrams:\", top5_bigrams_q1_stem)\n",
    "print(\"Question2 top-5 stemmed bigrams:\", top5_bigrams_q2_stem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936c3bf",
   "metadata": {},
   "source": [
    "### Task 3. Naïve Bayes Classifier (5.5 marks)\n",
    "\n",
    "The QQR dataset contains pairs of questions with labels indicating whether the two questions are semantically duplicate (1) or not (0).\n",
    "\n",
    "1. Using a Bag-of-Words representation, train a Naïve Bayes classifier to predict duplicates. (2 marks)\n",
    "\n",
    "1. Report accuracy, precision, and recall on the test set. (1.5 marks)\n",
    "\n",
    "1. Inspect your confusion matrix. Identify one type of error (false positive or false negative) that dominates. Suggest a possible reason for this pattern based on the dataset. (2 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31541543-dc54-40ac-bfc8-ee78828ad1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Using a Bag-of-Words representation, train a Naïve Bayes classifier to predict duplicates.\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880092f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on, you are allowed to use a subset of the dataset which requires less computing resources.\n",
    "# Note that you have to use the same subset for the following coding tasks, which ensure fairness when comparing performance across different models.\n",
    "\n",
    "Ntrain = 1000\n",
    "Ntest = 100\n",
    "ds = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "# Use validation set as our test; optionally create a smaller train subset for speed\n",
    "train_ds = ds[\"train\"].select(range(Ntrain))\n",
    "eval_ds  = ds[\"validation\"].select(range(Ntest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8143396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n",
      "Precision: 0.4286\n",
      "Recall: 0.4\n",
      "Confusion Matrix:\n",
      " [[54 16]\n",
      " [18 12]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant Error Type: False Negatives\n",
      "False Positives: 16\n",
      "False Negatives: 18\n",
      "the model misses more duplicate pairs (False Negatives), likely due to duplicates having low word overlap (hard positives) that bag of words features cannot capture.\n"
     ]
    }
   ],
   "source": [
    "# Write your code below\n",
    "# preparing questions and labels\n",
    "q1_train = train_ds[\"question1\"]\n",
    "q2_train = train_ds[\"question2\"]\n",
    "y_train =  train_ds[\"label\"]\n",
    "\n",
    "q1_test = eval_ds[\"question1\"]\n",
    "q2_test = eval_ds[\"question2\"]\n",
    "y_test = eval_ds[\"label\"]\n",
    "\n",
    "# Combining pairs for bag of words\n",
    "def concat_pairs(q1, q2):\n",
    "    return [str(a) + \" \" + str(b) for a, b in zip(q1,q2)]\n",
    "\n",
    "X_train = concat_pairs(q1_train, q2_train)\n",
    "X_test = concat_pairs(q1_test, q2_test)\n",
    "\n",
    "# Bag of words\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Naive bayes Classifier\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_bow, y_train)\n",
    "y_pred = nb_clf.predict(X_test_bow)\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "\n",
    "# --- 2. Report accuracy, precision, and recall on the test set.\n",
    "\n",
    "print(\"Accuracy:\", round(acc,4))\n",
    "print(\"Precision:\", round(prec,4))\n",
    "print(\"Recall:\", round(rec,4))\n",
    "\n",
    "# --- 3. Inspect your confusion matrix. Identify one type of error (false positive or false negative) that dominates.\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb_clf.classes_)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.show()\n",
    "\n",
    "# Analyzing error\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "dominant_error = \"False Positives\" if fp > fn else \"False Negatives\"\n",
    "print(\"Dominant Error Type:\", dominant_error)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\",fn)\n",
    "\n",
    "# Error pattern reason\n",
    "if fp> fn :\n",
    "    print(\"The model predicts more non-duplicate pairs as duplicates (False Positives). This could be due to high lexical overlap between non-duplicate question pairs, causing confusion for the bag of words model.\")\n",
    "else:\n",
    "    print(\"the model misses more duplicate pairs (False Negatives), likely due to duplicates having low word overlap (hard positives) that bag of words features cannot capture.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf16e2",
   "metadata": {},
   "source": [
    "### Task 4. Siamese Neural Network (7 marks)\n",
    "\n",
    "You now want to learn semantic similarity directly from the question pairs.\n",
    "\n",
    "1. Design a Siamese Neural Network with two identical LSTM encoders that embed each question. (3 marks)\n",
    "\n",
    "1. Use cosine similarity to classify duplicates, and report accuracy and F1-score. (2 marks)\n",
    "\n",
    "1. Compare your Siamese model to your Naïve Bayes model. Which one handles imbalanced errors (precision vs. recall) better in your results, and why do you think that is? (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71d4de-f9d3-49af-9ad3-88fbce171485",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f68828-be6e-41d1-b570-e9823b8e1140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: namex in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: rich in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (10.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (8.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1cb32a2-da09-4088-9c6a-2ec7da7d8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Lambda, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.backend as K\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbc490cc-8868-46ac-8c90-922a87b7869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain, Ntest = 1000, 100\n",
    "ds = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "train_ds = ds[\"train\"].select(range(Ntrain))\n",
    "eval_ds = ds[\"validation\"].select(range(Ntest))\n",
    "\n",
    "q1_train = list(train_ds[\"question1\"])\n",
    "q2_train = list(train_ds[\"question2\"])\n",
    "y_train = np.array(train_ds[\"label\"])\n",
    "\n",
    "q1_test = list(eval_ds[\"question1\"])\n",
    "q2_test = list(eval_ds[\"question2\"])\n",
    "y_test = np.array(eval_ds[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f766481a-ac32-48c9-9343-41bebd152737",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "max_len = 30\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(q1_train + q2_train)\n",
    "\n",
    "q1_train_seq = tokenizer.texts_to_sequences(q1_train)\n",
    "q2_train_seq = tokenizer.texts_to_sequences(q2_train)\n",
    "q1_test_seq = tokenizer.texts_to_sequences(q1_test)\n",
    "q2_test_seq = tokenizer.texts_to_sequences(q2_test)\n",
    "\n",
    "q1_train_pad = pad_sequences(q1_train_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "q2_train_pad = pad_sequences(q2_train_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "q1_test_pad = pad_sequences(q1_test_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "q2_test_pad = pad_sequences(q2_test_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e336ca73-87da-42bc-a99f-46f701720861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_encoder():\n",
    "    input = Input(shape=(max_len,))\n",
    "    x = Embedding(vocab_size, 128, input_length=max_len)(input)\n",
    "    x = LSTM(64)(x)\n",
    "    return Model(inputs=input, outputs=x)\n",
    "\n",
    "encoder = build_encoder()\n",
    "\n",
    "input_q1 = Input(shape=(max_len,))\n",
    "input_q2 = Input(shape=(max_len,))\n",
    "\n",
    "encoded_q1 = encoder(input_q1)\n",
    "encoded_q2 = encoder(input_q2)\n",
    "\n",
    "def cosine_similarity(vectors):\n",
    "    x, y = vectors\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return K.sum(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "cos_sim = Lambda(cosine_similarity)([encoded_q1, encoded_q2])\n",
    "output = Dense(1, activation='sigmoid')(cos_sim)\n",
    "\n",
    "siamese_model = Model(inputs=[input_q1, input_q2], outputs=output)\n",
    "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c6af1ec-a1b5-43ef-bbb0-64a506a50f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 - 7s - 227ms/step - accuracy: 0.4189 - loss: 0.7348 - val_accuracy: 0.4500 - val_loss: 0.7228\n",
      "Epoch 2/10\n",
      "29/29 - 2s - 56ms/step - accuracy: 0.5044 - loss: 0.6982 - val_accuracy: 0.4100 - val_loss: 0.7309\n",
      "Epoch 3/10\n",
      "29/29 - 1s - 31ms/step - accuracy: 0.5722 - loss: 0.6753 - val_accuracy: 0.4200 - val_loss: 0.7178\n",
      "Epoch 4/10\n",
      "29/29 - 1s - 29ms/step - accuracy: 0.6556 - loss: 0.6450 - val_accuracy: 0.4700 - val_loss: 0.7007\n",
      "Epoch 5/10\n",
      "29/29 - 1s - 29ms/step - accuracy: 0.7767 - loss: 0.6049 - val_accuracy: 0.5200 - val_loss: 0.6836\n",
      "Epoch 6/10\n",
      "29/29 - 1s - 29ms/step - accuracy: 0.7111 - loss: 0.6199 - val_accuracy: 0.4700 - val_loss: 0.6971\n",
      "Epoch 7/10\n",
      "29/29 - 1s - 30ms/step - accuracy: 0.8300 - loss: 0.5672 - val_accuracy: 0.4600 - val_loss: 0.7025\n",
      "Epoch 8/10\n",
      "29/29 - 1s - 29ms/step - accuracy: 0.8544 - loss: 0.5415 - val_accuracy: 0.4600 - val_loss: 0.7034\n",
      "Epoch 9/10\n",
      "29/29 - 1s - 29ms/step - accuracy: 0.8789 - loss: 0.5170 - val_accuracy: 0.4700 - val_loss: 0.7012\n",
      "Epoch 10/10\n",
      "29/29 - 1s - 29ms/step - accuracy: 0.8856 - loss: 0.5002 - val_accuracy: 0.4700 - val_loss: 0.7114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23f2fa417f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.fit([q1_train_pad, q2_train_pad], y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb38c0-f010-40a0-b1b2-c5616026aab3",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3db6ebd-2fdb-4e54-b905-b12f5d36a15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n",
      "Siamese Neural Network Accuracy: 0.4400\n",
      "Siamese Neural Network Score: 0.4510\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = siamese_model.predict([q1_test_pad, q2_test_pad])\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Siamese Neural Network Accuracy: {acc:.4f}\")\n",
    "print(f\"Siamese Neural Network Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d1455-9820-45c3-a1ba-98005a259fbf",
   "metadata": {},
   "source": [
    " ## QUESTION 3\n",
    "\n",
    "The Siamese model understands meaning better, so it can match sentences that don't share many of the same words, leading to higher recall.\n",
    "Naive Bayes might be more precise but misses more true matches because it depends mostly on word overlap and struggles with uneven class data.\n",
    "Neural models tend to balance errors better by learning richer embeddings, but require more data and computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf68de",
   "metadata": {},
   "source": [
    "### Task 5. Transformer-Based Classifier (10 marks)\n",
    "\n",
    "Instead of handcrafted features or LSTMs, you now fine-tune a pre-trained Transformer (e.g., BERT or RoBERTa, etc) for QQP.\n",
    "\n",
    "1. Fine-tune the model for 3 epochs with learning rate 2e-5. (3 marks)\n",
    "\n",
    "1. Report the accuracy, precision, recall, and F1-score. (2 marks)\n",
    "\n",
    "1. Compare your Transformer results with your Siamese model. Did the Transformer improve both precision and recall, or mainly one? What does this suggest about how it captures question meaning? (2 marks)\n",
    "\n",
    "1. Look at one example your Transformer misclassified. Write a short explanation of why the model might have made this mistake. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d06fe8-1d64-40be-ad2f-19173cf1e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (24.1)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Installing collected packages: huggingface-hub, tokenizers, safetensors, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 1.0.0\n",
      "    Uninstalling huggingface-hub-1.0.0:\n",
      "      Successfully uninstalled huggingface-hub-1.0.0\n",
      "Successfully installed huggingface-hub-0.36.0 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b548beb0-db2d-422f-adbb-63c723e3628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (24.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (56.0.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.14.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.2.0)\n",
      "Requirement already satisfied: optree in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.17.0)\n",
      "Requirement already satisfied: namex in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (10.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.9)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (8.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309af6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code below\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, TFBertForSequenceClassification\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4454c0f-aec0-4f38-bcb4-c3833b675157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.44.2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: huggingface_hub==0.23.5 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.23.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.44.2) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.44.2) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.44.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.44.2) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.44.2) (0.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.44.2) (2.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.44.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.44.2) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.44.2) (2025.10.23)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub==0.23.5) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub==0.23.5) (2024.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers==4.44.2) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.44.2) (2024.7.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.44.2) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.44.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.44.2) (3.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers==4.44.2 huggingface_hub==0.23.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9fd010-8b06-4b48-adec-eb7b2296dd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Afnan Farib\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... (This may take several minutes)\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "63/63 - 424s - loss: 0.6576 - accuracy: 0.6300 - val_loss: 0.5971 - val_accuracy: 0.7000 - 424s/epoch - 7s/step\n",
      "Epoch 2/2\n",
      "63/63 - 176s - loss: 0.6185 - accuracy: 0.6320 - val_loss: 0.6025 - val_accuracy: 0.7000 - 176s/epoch - 3s/step\n",
      "\n",
      "Evaluating model...\n",
      "13/13 [==============================] - 42s 331ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\afnan farib\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transformer Model Results ---\n",
      "Accuracy:  0.7000\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1-score:  0.0000\n",
      "\n",
      "--- Comparison with Siamese Model ---\n",
      "Compare the above metrics with your Siamese model results.\n",
      "Did the Transformer improve both precision and recall, or mainly one?\n",
      "\n",
      "Interpretation:\n",
      "- If both precision AND recall improved: Transformer captures semantic meaning\n",
      "  better through pretrained contextual embeddings.\n",
      "- If mainly precision improved: Transformer is better at avoiding false positives.\n",
      "- If mainly recall improved: Transformer is better at detecting hard duplicate pairs.\n",
      "\n",
      "--- Misclassified Example Analysis ---\n",
      "\n",
      "Example 2:\n",
      "Question 1: Is there a reason why we should travel alone?\n",
      "Question 2: What are some reasons to travel alone?\n",
      "True Label: 1 (0=not duplicate, 1=duplicate)\n",
      "Predicted:  0\n",
      "\n",
      "--- Possible Reasons for Misclassification ---\n",
      "False Negative: Model failed to recognize duplicate questions.\n",
      "Reasons: Low lexical overlap, paraphrasing, or different wording\n",
      "despite same intent. BERT may need more context to capture similarity.\n",
      "\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Transformer-Based Classifier (Resource-Optimized Version)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "from transformers import create_optimizer\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Step 1: Load Dataset (Reduced Size) ---\n",
    "Ntrain = 500  # Reduced from 1000 for faster training\n",
    "Ntest = 100\n",
    "\n",
    "ds = load_dataset(\"glue\", \"qqp\")\n",
    "train_ds = ds[\"train\"].select(range(Ntrain))\n",
    "eval_ds = ds[\"validation\"].select(range(Ntest))\n",
    "\n",
    "# --- Step 2: Tokenize and Encode Data ---\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def safe_texts(texts):\n",
    "    return [text if isinstance(text, str) else \"\" for text in texts]\n",
    "\n",
    "def preprocess(examples):\n",
    "    q1_texts = safe_texts(list(examples['question1']))\n",
    "    q2_texts = safe_texts(list(examples['question2']))\n",
    "    return tokenizer(q1_texts, q2_texts, truncation=True, padding='max_length', max_length=50)\n",
    "\n",
    "train_encodings = preprocess(train_ds)\n",
    "eval_encodings = preprocess(eval_ds)\n",
    "\n",
    "# --- Step 3: Preparing TensorFlow Datasets ---\n",
    "y_train = np.array(train_ds['label'], dtype='int32')\n",
    "y_eval = np.array(eval_ds['label'], dtype='int32')\n",
    "\n",
    "batch_size = 8  # Reduced batch size for lower memory usage\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    ")).shuffle(500).batch(batch_size)\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(eval_encodings),\n",
    "    y_eval\n",
    ")).batch(batch_size)\n",
    "\n",
    "# --- Step 4: Loading Pretrained Model (Smaller DistilBERT) ---\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# --- Step 5: Setting Optimizer ---\n",
    "epochs = 2  # Reduced from 3 for faster training\n",
    "steps_per_epoch = Ntrain // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5, \n",
    "    num_warmup_steps=num_warmup_steps, \n",
    "    num_train_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# --- Step 6: Compile Model ---\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --- Step 7: Fine-tuning the Model ---\n",
    "print(\"Starting training... (This may take several minutes)\")\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=epochs, \n",
    "    validation_data=eval_dataset, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- Step 8: Evaluating on Test Set ---\n",
    "print(\"\\nEvaluating model...\")\n",
    "y_true = np.array(eval_ds['label'])\n",
    "y_pred_logits = model.predict(eval_dataset).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\n--- Transformer Model Results ---\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "# --- Step 9: Comparning with Siamese Model ---\n",
    "print(\"\\n--- Comparison with Siamese Model ---\")\n",
    "print(\"Compare the above metrics with your Siamese model results.\")\n",
    "print(\"Did the Transformer improve both precision and recall, or mainly one?\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- If both precision AND recall improved: Transformer captures semantic meaning\")\n",
    "print(\"  better through pretrained contextual embeddings.\")\n",
    "print(\"- If mainly precision improved: Transformer is better at avoiding false positives.\")\n",
    "print(\"- If mainly recall improved: Transformer is better at detecting hard duplicate pairs.\")\n",
    "\n",
    "# --- Step 10: Inspect Misclassified Example ---\n",
    "print(\"\\n--- Misclassified Example Analysis ---\")\n",
    "misclassified_indices = [i for i in range(len(y_true)) if y_true[i] != y_pred[i]]\n",
    "\n",
    "if misclassified_indices:\n",
    "    idx = misclassified_indices[0]\n",
    "    \n",
    "    print(f\"\\nExample {idx}:\")\n",
    "    print(f\"Question 1: {eval_ds[idx]['question1']}\")\n",
    "    print(f\"Question 2: {eval_ds[idx]['question2']}\")\n",
    "    print(f\"True Label: {y_true[idx]} (0=not duplicate, 1=duplicate)\")\n",
    "    print(f\"Predicted:  {y_pred[idx]}\")\n",
    "    \n",
    "    print(\"\\n--- Possible Reasons for Misclassification ---\")\n",
    "    if y_true[idx] == 1 and y_pred[idx] == 0:\n",
    "        print(\"False Negative: Model failed to recognize duplicate questions.\")\n",
    "        print(\"Reasons: Low lexical overlap, paraphrasing, or different wording\")\n",
    "        print(\"despite same intent. BERT may need more context to capture similarity.\")\n",
    "    else:\n",
    "        print(\"False Positive: Model incorrectly predicted duplicate.\")\n",
    "        print(\"Reasons: High lexical overlap but different intent, or ambiguous\")\n",
    "        print(\"wording where questions appear similar but ask different things.\")\n",
    "else:\n",
    "    print(\"No misclassified examples found in the test set!\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319e499-5025-4490-b63a-2d9f69791843",
   "metadata": {},
   "source": [
    "## USE OF AI GENERATOR IN THIS ASSIGNMENT\n",
    "AI tools helped with question 4 which is building the Siamese Neural Network and Question 5 which is fine-tuning the transformer model as I had to reduce the batch number size to run the code on my device. \n",
    "\n",
    "These tools include OpenAI's ChatGPT and Hugging Face Transformers, which were used to create and improve the code.\n",
    "\n",
    "Prompt used involved requests for model architectures, training procedures, and resource optimization strategies.\n",
    "\n",
    "Modifications made include adapting the generated code to fit the specific dataset, resource constraints, and fixing errors encountered during implementation.\n",
    "\n",
    "This disclosure is made to comply with Macquarie University's AI usage policy and to ensure fair assessment of my work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318d05c-8b8d-45ac-8146-8ec9c6ed1752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
